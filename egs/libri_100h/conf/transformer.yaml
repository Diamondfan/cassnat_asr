#model configuration
model_type: "conformer"
input_size: 80
N_enc: 12
N_dec: 6
d_model: 512 #256
d_ff: 2048          
n_head: 8 #4
dropout: 0.1
ctc_alpha: 0.5
interctc_alpha: 0.5
interctc_layer: 6
label_smooth: 0.1
disable_ls: False
resume_model: ""
use_BERT_tokenizer: True
tokenizer: "data/dict/bpemodel_unigram_1024.model"
bert_name: "bert-base-uncased"
bert_path: "/data/ruchao/workdir/language_model/bert/" 

#conformer related conf
use_conv_enc: True
share_ff: False
d_encff: 1024
d_decff: 2048
pos_type: "relative"
enc_max_relative_len: 20
enc_kernel_size: 31

n_features: 80
vocab_size: 4990
left_ctx: 0
right_ctx: 0
skip_frame: 1

#noam warm up
warmup_type: "noam_warmup"  # noam_warmup, custom_exp, custom_linear
noam_factor: 0.002               # learning rate for noam_wamup, factor for custom_exp
warmup_steps: 15000
total_steps: 250000           # for custom_linear

#cosine learning schedule
cosine_total: 100000
cosine_warmup: 1000

#Double, multistep
decay_rate: 0.01
s_warm: 2000
s_decay: 50000
s_keep: 200000

#normal
learning_rate: 0.001
min_lr: 0.000002
patience: 2
anneal_lr_ratio: 0.5

# dataloader
dataset_type: "DynamicDataset"   #"SpeechDataset, DynamicDataset"
max_len: 5000
batch_size: 16

batch_type: "utterance"
max_frmlen: 800 
max_lablen: 150 
filter_max: 5000
filter_min: 0

padding_idx: 0
accum_grad: 2
grad_clip: 5
weight_decay: 0

use_gpu: True

use_cmvn: True
use_specaug: True
spec_aug:
  use_time_warp: False
  resize_mode: "PIL"         #time warp is temporarily deactivated
  max_time_warp: 5
  n_freq_mask: 2
  max_freq_width: 27 #30
  n_time_mask: 10  #2
  max_time_width: 0.05  #40
  inplace: True
  replace_with_zero: False

