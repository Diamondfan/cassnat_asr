vocab_file: data/dict/vocab_wp.txt
global_cmvn: data/fbank/cmvn_sp.ark
use_cmvn: True
use_BERT_tokenizer: False
tokenizer: "data/dict/bpemodel_unigram_1024.model"
bert_name: "bert-base-uncased"
bert_path: "/data/ruchao/workdir/language_model/bert/" 

#model configuration
model_type: "hubert"
input_size: 80
N_extra: 1
d_model: 768 #512
d_ff: 3072 #2048
n_head: 12 #8
dropout: 0

#hubert layers
layer_type: "transformer"
encoder_ffn_embed_dim: 3072
encoder_attention_heads: 12
attention_dropout: 0
activation_dropout: 0.0
activation_fn: 'gelu'
layer_norm_first: False
depthwise_conv_kernel_size: 31
attn_type: ''
dropout: 0
encoder_embed_dim: 768
required_seq_len_multiple: 2
# pos_conv_depth: 1
conv_pos: 128
conv_pos_groups: 16
encoder_layers: 12
layer_norm_first: False
encoder_layerdrop: 0
conv_feature_layers: '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2'

mask_prob: 0.0
mask_selection: 'static'
mask_other: 0.0
mask_length: 10
no_mask_overlap: False
mask_min_space: 1
mask_channel_prob: 0.0
mask_channel_selection: 'static'
mask_channel_other: 0.0
mask_channel_length: 10
no_mask_channel_overlap: False
mask_channel_min_space: 1
dropout_input: 0
feature_grad_mult: 0

n_features: 80
vocab_size: 4990
left_ctx: 0
right_ctx: 0
skip_frame: 1
padding_idx: -1
text_padding_idx: 0

use_gpu: True
dataset_type: "HubertDataset"   #"SpeechDataset, DynamicDataset"
max_len: 5000
batch_size: 1
sample_rate: 16000
normalize: False

batch_type: "utterance"
max_frmlen: 1000 
max_lablen: 100 
filter_max: 5000
filter_min: 0
max_samplen: 500000  #392400

#beam_decode
decode_type: 'esa'
max_decode_ratio: 0
beam_width: 1
ctc_beam: 1 #20
ctc_pruning: 0 #30
ctc_lp: 0 #2
ctc_lm_weight: 0.1 #used to load lm for ranking ESA
length_penalty: 0
T: 1.0

#nat related conf
use_trigger: True
src_trigger: False
use_unimask: False
left_trigger: 1
right_trigger: 1
use_best_path: False
num_iters:  1 #2
sample_num: 16 #8
sample_num2: 1 #8
threshold: 0.9

use_mlm: False 
apply_mask: False
test_hitrate: False
save_embedding: False
print_utt2diff: False
