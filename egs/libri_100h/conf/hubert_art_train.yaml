model_type: "hubert"
input_size: 80
N_dec: 6
d_model: 512 #256
d_ff: 2048          
d_decff: 2048
n_head: 8 #4
dropout: 0.1
ctc_alpha: 0.5
interctc_alpha: 0.5
interctc_layer: 6
label_smooth: 0.1
disable_ls: False
resume_model: "../../../pretrained_models/hubert/hubert_base_ls960.pt"
init_encoder: True
fix_encoder: False
use_BERT_tokenizer: False #True
tokenizer: "data/dict/bpemodel_unigram_1024.model"
bert_name: "bert-base-uncased"
bert_path: "/data/ruchao/workdir/language_model/bert/"

#hubert layers
layer_type: "transformer"
encoder_ffn_embed_dim: 3072
encoder_attention_heads: 12
attention_dropout: 0.1
activation_dropout: 0.0
activation_fn: 'gelu'
layer_norm_first: False
depthwise_conv_kernel_size: 31
attn_type: ''
dropout: 0.1
encoder_embed_dim: 768
required_seq_len_multiple: 2
# pos_conv_depth: 1
conv_pos: 128
conv_pos_groups: 16
encoder_layers: 12
layer_norm_first: False
encoder_layerdrop: 0
conv_feature_layers: '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2'

mask_prob: 0.60
mask_selection: 'static'
mask_other: 0.0
mask_length: 10
no_mask_overlap: False
mask_min_space: 1
mask_channel_prob: 0.50
mask_channel_selection: 'static'
mask_channel_other: 0.0
mask_channel_length: 64 #10
no_mask_channel_overlap: False
mask_channel_min_space: 1
dropout_input: 0.1
feature_grad_mult: 0

n_features: 80
vocab_size: 4999
left_ctx: 0
right_ctx: 0
skip_frame: 1

multi_optim: True
warmup_type: "noam_warmup"  # noam_warmup, custom_exp, custom_linear
noam_factor: [0.00005, 0.001, 0.001]       #[src_embed, encoder, ctc_gen and interctc gen,  decoders, text_encoder]  # learning rate for noam_wamup, factor for custom_exp
warmup_steps: [10000, 10000, 10000]
freeze_steps: [0, 0, 0]
total_steps: [25000, 250000, 25000]         # for custom_linear

decay_rate: 0.01
s_warm: 1000
s_decay: 40000
s_keep: 160000

cosine_total: 100000
cosine_warmup: 1000

#normal
learning_rate: 0.02
min_lr: 0.000002
patience: 2
anneal_lr_ratio: 0.5

# dataloader
dataset_type: "HubertDataset"   #"SpeechDataset, DynamicDataset"
max_len: 5000
batch_size: 1
sample_rate: 16000
normalize: False

batch_type: "samples"
max_frmlen: 1000 
max_lablen: 100 
filter_max: 5000
filter_min: 0

max_samplen: 1280000  #392400

padding_idx: -1
text_padding_idx: 0
accum_grad: 4
grad_clip: 5
weight_decay: 0

use_gpu: True
use_fp16: True

use_specaug: False
specaug_start_epoch: 100000
spec_aug:
  use_time_warp: False
  resize_mode: "PIL"
  max_time_warp: 5
  max_freq_width: 27
  n_freq_mask: 2
  max_time_width: 0.05
  n_time_mask: 10
  inplace: True
  replace_with_zero: False
