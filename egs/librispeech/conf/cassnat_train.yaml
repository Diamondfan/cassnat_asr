#model configuration
model_type: "transformer"
input_size: 240
N_enc: 12
N_extra: 1
N_self_dec: 5 # 3
N_mix_dec: 2 # 4
d_model: 512 #256
d_ff: 2048          
n_head: 8 #4
dropout: 0.1

#conformer related conf
share_ff: False
d_encff: 1024
d_decff: 1024 #2048
pos_type: "relative"
enc_max_relative_len: 20
enc_kernel_size: 31
use_conv_dec: True
dec_max_relative_len: 8
dec_kernel_size: 3

#nat related conf
use_trigger: True
use_src: True
src_trigger: True
use_unimask: False
left_trigger: 0
right_trigger: 0
use_best_path: False
sample_dist: 0
sample_topk: 0 #4
MWER_training: False
sample_num: 0

n_features: 80
vocab_size: 4999
left_ctx: 0
right_ctx: 2
skip_frame: 1

noam_factor: 10.0
noam_warmup: 40000 #12500

decay_rate: 0.01
s_warm: 1000
s_decay: 40000
s_keep: 160000

cosine_total: 100000
cosine_warmup: 1000
padding_idx: 0
accum_grad: 2
grad_clip: 5

use_gpu: True

use_specaug: True
specaug_start_epoch: 0
spec_aug:
  use_time_warp: False
  resize_mode: "PIL"
  max_time_warp: 5
  max_freq_width: 27
  n_freq_mask: 2
  max_time_width: 0.05
  n_time_mask: 10
  inplace: True
  replace_with_zero: False

