#model config
model_type: "transformer"
input_size: 240 
N_enc: 12
d_model: 512 #256
d_ff: 2048    
n_head: 8 #4
dropout: 0.1 

#conformer related conf
share_ff: False
d_encff: 1024
d_decff: 2048
pos_type: "relative"
max_relative_len: 20
kernel_size: 31

n_features: 80
vocab_size: 4990
left_ctx: 0
right_ctx: 2
skip_frame: 1

#noam warm up
warmup_type: 'noam_warmup'   # noam_warmup, custom_exp, custom_linear
noam_factor: 0.002           # learning rate for noam_warmup, factor for custom_exp
warmup_steps: 10000 
total_steps: 100000           # for custom_linear

#cosine learning schedule
cosine_total: 100000
cosine_warmup: 1000

#Double, multistep
decay_rate: 0.01
s_warm: 2000
s_decay: 40000
s_keep: 160000

batch_type: "utterance"   #utterance or frame
batch_size: 16            #not used for iterable dataset
batch_num: 16 #24000

padding_idx: 0
accum_grad: 2
grad_clip: 5

use_gpu: True

use_specaug: False
spec_aug:
  resize_mode: "PIL"         #time warp is temporarily deactivated
  max_time_warp: 0
  max_freq_width: 27
  n_freq_mask: 2
  max_time_width: 0.05
  n_time_mask: 10
  inplace: True
  replace_with_zero: False
