#model configuration
model_type: "conformer"
input_size: 80
N_enc: 12
N_extra: 1
d_model: 512
d_ff: 2048
n_head: 8
dropout: 0.1
ctc_alpha: 0.5
interctc_alpha: 0.5
interctc_layer: 6
att_alpha: 1
interce_alpha: 0.1
interce_layer: 6
label_smooth: 0.1
disable_ls: False
resume_model: ""
init_encoder: False #True
fix_encoder: False
use_BERT_tokenizer: False #True
tokenizer: "data/dict/bpemodel_unigram_500.model"

#conformer related conf
use_conv_enc: True
share_ff: False
d_encff: 1024
d_decff: 1024 #2048
pos_type: "relative"
enc_max_relative_len: 20
enc_kernel_size: 31
use_conv_dec: True
dec_max_relative_len: 8
dec_kernel_size: 3

#nat related conf
use_trigger: True
src_trigger: False
use_unimask: False
left_trigger: 1
right_trigger: 1
sample_topk: 0 
sample_num: 0
save_embedding: False

n_features: 80
vocab_size: 4999
left_ctx: 0
right_ctx: 0
skip_frame: 1

#noam warm up
warmup_type: "noam_warmup"  # noam_warmup, custom_exp, custom_linear
noam_factor: [0.001, 0.001, 0.001]       #[encoder, ctc_gen and interctc gen,  decoders]  # learning rate for noam_wamup, factor for custom_exp
warmup_steps: [25000, 25000, 25000]
freeze_steps: [0, 0, 0]
total_steps: [250000, 250000, 250000]           # for custom_linear

# multistep
decay_rate: 0.01
s_warm: 1000
s_decay: 40000
s_keep: 160000

cosine_total: 100000
cosine_warmup: 1000

#normal
learning_rate: 0.0002
min_lr: 0.000002
patience: 2
anneal_lr_ratio: 0.5

# dataloader
dataset_type: "DynamicDataset"   #"SpeechDataset, DynamicDataset"
max_len: 5000
batch_size: 16

batch_type: "utterance"
max_frmlen: 1000 
max_lablen: 100 
filter_max: 5000
filter_min: 50

padding_idx: 0
text_padding_idx: 0
accum_grad: 4
grad_clip: 5
weight_decay: 0

use_gpu: True
use_cmvn: True
use_fp16: True

use_specaug: True
specaug_start_epoch: 0
spec_aug:
  use_time_warp: False
  resize_mode: "PIL"
  max_time_warp: 5
  max_freq_width: 27
  n_freq_mask: 2
  max_time_width: 0.05
  n_time_mask: 10
  inplace: True
  replace_with_zero: False

# mask strategy
multi_ctc: True
multi_ctc_alpha: 1.0
mask_prob: 0
use_mlm: False #True
apply_mask: False #True
use_mask_embed: False #True
mlm_alpha: 0 #0.1
dec_mask_prob: 0.15
dec_mask_selection: 'uniform' 
dec_min_masks: 1
dec_mask_other: 1
dec_mask_length: 3
dec_mask_min_space: 1
dec_mask_dropout: 0.0
dec_require_same_masks: False #True
dec_no_mask_overlap: False
 


