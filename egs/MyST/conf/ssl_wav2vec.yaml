#model configuration
model_type: "conformer"
input_size: 80
N_enc: 12
d_model: 512 #256
d_ff: 2048          
n_head: 8 #4
dropout: 0.1
resume_model: ""
dropout_input: 0
dropout_features: 0

#conformer related conf
use_conv_enc: True
share_ff: False
d_encff: 1024
pos_type: "relative"
enc_max_relative_len: 20
enc_kernel_size: 31

n_features: 80
vocab_size: 4990
left_ctx: 0
right_ctx: 0
skip_frame: 1

#noam warm up
warmup_type: "noam_warmup"  # noam_warmup, custom_exp, custom_linear
noam_factor: 0.0005               # learning rate for noam_wamup, factor for custom_exp
warmup_steps: 25000
total_steps: 250000           # for custom_linear

#cosine learning schedule
cosine_total: 100000
cosine_warmup: 1000

#Double, multistep
decay_rate: 0.01
s_warm: 2000
s_decay: 50000
s_keep: 200000

#normal
learning_rate: 0.001
min_lr: 0.000002
patience: 2
anneal_lr_ratio: 0.5

# dataloader
dataset_type: "DynamicDataset"   #"SpeechDataset, DynamicDataset"
max_len: 5000
batch_size: 16

batch_type: "utterance"
max_frmlen: 800 
max_lablen: 150 
filter_max: 5000
filter_min: 200

padding_idx: 0
accum_grad: 12
grad_clip: 5
weight_decay: 0.01
eps: 0.000001

use_gpu: True

use_cmvn: True

# criterion
infonce: True
loss_weights: [0.1]
log_keys: ["prob_perplexity","code_perplexity","temp"]

# mask strategy
mask_prob: 0.65
mask_selection: 'static' 
mask_other: 0
mask_length: 10
mask_min_space: 1
mask_dropout: 0.0
require_same_masks: True
no_mask_overlap: False
    
mask_channel_prob: 0
mask_channel_before: False
mask_channel_selection: 'static'
mask_channel_other: 0
mask_channel_length: 64
no_mask_channel_overlap: False
mask_channel_min_space: 1

# sample negatives
num_negatives: 100
cross_sample_negatives: 0
codebook_negatives: 0
negatives_from_everywhere: False

# vq layer
logit_temp: 1
quantize_targets: True
quantizer_depth: 1
quantizer_factor: 3
latent_vars: 320
latent_groups: 2
latent_dim: 0
latent_temp: (2, 0.5, 0.999995)
final_dim: 256


